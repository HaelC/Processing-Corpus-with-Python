{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, welcome to this notebook. You can run the following cells step by step, and hopefully you will know the basis of processing corpus with Python. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Single File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'LCMC_A.xml'\n",
    "with open(filename, encoding='utf-8') as f:\n",
    "    read_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line declares a variable called `filename`.  \n",
    "The second line tells the system to read the file. Note that `encoding='utf-8'` indicates the encoding of the file, and currently *utf-8* is the most frequently used encoding. If you don’t know the file’s encoding, take utf-8 by default. (Later I’ll write how to dealing with other encodings.)  \n",
    "The third line just read the entire content and pass it to a variable `read_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following cell to see whether it's successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<LCMC ver=\"character\"><header><corpusDesc><corpusName>The Lancaster Corpus of Mandarin Chinese </corpusName><creator>Created by the Department of Linguistics, Lancaster University </creator><funding>Funded by the Economic and social Research Council (ESRC), UK </funding><designer>Designed by Anthony McEnery and Zhonghua Xiao </designer><supervision>Supervised by Anthony McEnery </supervision><textcollect>Texts collected by Zhonghua Xiao </textcollect><proofread>Electronic texts proofread and corrected by Zhonghua Xiao and Xin Huang </proofread><POStag>Segmented and POS-tagged by Zhonghua Xiao </POStag><unicodify>Converted into Unicode by Multilingual Corpus Tools (MLCT) developed by Scott Piao and Andrew Wilson </unicodify></corpusDesc><publication><publisher>Department of Linguistics, Lancaster University, LA1 4YT, UK </publisher><availability region=\"world\"></availability><pubDate>June 2003 </pubDate><contact>Anthony McEnery, a.mcenery@lancaster'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data[:1000]               # print the first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Useful Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tag = re.findall(r'<w POS=\"(\\w{0,4})\">.{0,7}</w>', read_data)\n",
    "token = re.findall(r'<w POS=\"\\w{0,4}\">(.{0,7})</w>', read_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most useful information is the tags and the tokens. With the above code we can extract these parts parallelly (in two seperate lists). Here I use the regular expression (re) and maybe it is a bit abstruce. Well, you just need to know what it does: recall the word form: `<w POS=\"a\">大</w>`, the second line extract the content between the quotes(`a`, which is a tag); the third line extract the content between the right angle bracket and the left angle bracket(`大`, which is a token).  \n",
    "Note that with the above codes I just extract words, not including punctuations. If you need to treat punctuations as tokens, use the following code instead:  \n",
    "`tag = re.findall(r'<[wc] POS=\"(\\w{0,4})\">.{0,7}</[wc]>', read_data)`,  \n",
    "`token = re.findall(r'<[wc] POS=\"\\w{0,4}\">(.{0,7})</[wc]>', read_data)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter = collections.Counter(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 4108),\n",
       " ('了', 1267),\n",
       " ('在', 993),\n",
       " ('一', 771),\n",
       " ('是', 765),\n",
       " ('和', 597),\n",
       " ('他', 530),\n",
       " ('说', 373),\n",
       " ('上', 361),\n",
       " ('着', 359)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have extract the useful part, we can count the frequencies. The first line import a library `collections` for counting. The second line counts the token list and the third line outputs 10 most frequently-used words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Target Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the freguency just gives us an overview. Then let's focus on our target words. We need to know the word's occurrence in the corpus. For example, the following line helps to find all indices of the word '是'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, x in enumerate(token) if x == '是']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the indices we get, now we can perform a variety of tasks. We can see the concordance of the word, n-gram, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_token = [token[i-1] for i in indices]\n",
    "next_token = [token[i+1] for i in indices]\n",
    "\n",
    "pre_tag = [tag[i-1] for i in indices]\n",
    "next_tag = [tag[i+1] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines stores the tokens before '是' and the tokens next '是' separately, and the last two lines stores the tags before '是' and the tags next '是'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
